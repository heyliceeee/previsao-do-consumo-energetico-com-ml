Beginning AutoGluon training... Time limit = 3200s
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\AutoML_lag\modelo'
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.70 GB / 15.93 GB (16.9%)
Disk Space Avail:   690.27 GB / 931.46 GB (74.1%)
===================================================
Setting presets to: best

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': 'default',
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 2,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3200,
 'verbosity': 2}

Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.

Provided data contains following columns:
	target: 'EnergyNormalized'
	known_covariates:
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
	past_covariates:
		categorical:        ['Season']
		continuous (float): ['ActiveEnergy(kWh)', 'Day', 'Month', 'Year', 'IsWeekend', 'IsHoliday', ...]
	static_features:
		categorical:        []
		continuous (float): ['PopulationDensity']

AutoGluon will ignore following non-numeric/non-informative columns:
	ignored covariates:      ['Date']

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================

Starting training. Start time is 2025-05-23 10:30:49
Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']
Training timeseries model SeasonalNaive. Training for up to 246.2s of the 3200.0s of remaining time.
	Time limit exceeded... Skipping SeasonalNaive.
Training timeseries model RecursiveTabular. Training for up to 266.6s of the 3199.6s of remaining time.
	-0.2035       = Validation score (-WQL)
	1.60    s     = Training runtime
	1.57    s     = Validation (prediction) runtime
Training timeseries model DirectTabular. Training for up to 290.6s of the 3196.4s of remaining time.
	-0.0436       = Validation score (-WQL)
	85.86   s     = Training runtime
	0.53    s     = Validation (prediction) runtime
Training timeseries model NPTS. Training for up to 311.0s of the 3110.0s of remaining time.
	Time limit exceeded... Skipping NPTS.
Training timeseries model DynamicOptimizedTheta. Training for up to 345.5s of the 3109.8s of remaining time.
	Time limit exceeded... Skipping DynamicOptimizedTheta.
Training timeseries model AutoETS. Training for up to 388.7s of the 3109.6s of remaining time.
	Time limit exceeded... Skipping AutoETS.
Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 444.2s of the 3109.4s of remaining time.
	-0.0460       = Validation score (-WQL)
	3.42    s     = Training runtime
	4.08    s     = Validation (prediction) runtime
Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 517.0s of the 3101.8s of remaining time.
	Fine-tuning on the CPU detected. We recommend using a GPU for faster fine-tuning of Chronos.
	Warning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.
	TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
Training timeseries model TemporalFusionTransformer. Training for up to 616.9s of the 3067.8s of remaining time.
	Warning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.
	name 'exit' is not defined
Training timeseries model DeepAR. Training for up to 764.3s of the 2893.0s of remaining time.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	name 'exit' is not defined
Training timeseries model PatchTST. Training for up to 1145.4s of the 2890.8s of remaining time.
	Warning: Exception caused PatchTST to fail during training... Skipping this model.
	name 'exit' is not defined
Training timeseries model TiDE. Training for up to 2290.5s of the 2890.5s of remaining time.
	-0.0322       = Validation score (-WQL)
	375.67  s     = Training runtime
	0.07    s     = Validation (prediction) runtime
Fitting simple weighted ensemble.
	Ensemble weights: {'DirectTabular': 0.56, 'RecursiveTabular': 0.11, 'TiDE': 0.32}
	-0.0239       = Validation score (-WQL)
	0.29    s     = Training runtime
	2.18    s     = Validation (prediction) runtime
Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'ChronosZeroShot[bolt_base]', 'TiDE', 'WeightedEnsemble']
Total runtime: 685.54 s
Best model: WeightedEnsemble
Best model score: -0.0239
Beginning AutoGluon training... Time limit = 3200s
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\AutoML_lag\modelo'
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.59 GB / 15.93 GB (16.3%)
Disk Space Avail:   690.20 GB / 931.46 GB (74.1%)
===================================================
Setting presets to: best

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': 'default',
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 2,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3200,
 'verbosity': 2}

Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.

Provided data contains following columns:
	target: 'EnergyNormalized'
	known_covariates:
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
	past_covariates:
		categorical:        ['Season']
		continuous (float): ['ActiveEnergy(kWh)', 'Day', 'Month', 'Year', 'IsWeekend', 'IsHoliday', ...]
	static_features:
		categorical:        []
		continuous (float): ['PopulationDensity']

AutoGluon will ignore following non-numeric/non-informative columns:
	ignored covariates:      ['Date']

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================

Starting training. Start time is 2025-05-23 10:46:07
Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']
Training timeseries model SeasonalNaive. Training for up to 245.9s of the 3197.2s of remaining time.
	Time limit exceeded... Skipping SeasonalNaive.
Training timeseries model RecursiveTabular. Training for up to 266.4s of the 3196.9s of remaining time.
	-0.2035       = Validation score (-WQL)
	1.44    s     = Training runtime
	0.85    s     = Validation (prediction) runtime
Training timeseries model DirectTabular. Training for up to 290.4s of the 3194.6s of remaining time.
	-0.0435       = Validation score (-WQL)
	71.25   s     = Training runtime
	0.63    s     = Validation (prediction) runtime
Training timeseries model NPTS. Training for up to 312.3s of the 3122.7s of remaining time.
	Time limit exceeded... Skipping NPTS.
Training timeseries model DynamicOptimizedTheta. Training for up to 346.9s of the 3122.4s of remaining time.
	Time limit exceeded... Skipping DynamicOptimizedTheta.
Training timeseries model AutoETS. Training for up to 390.3s of the 3122.2s of remaining time.
	Time limit exceeded... Skipping AutoETS.
Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 446.0s of the 3122.0s of remaining time.
	-0.0460       = Validation score (-WQL)
	16.18   s     = Training runtime
	1.09    s     = Validation (prediction) runtime
Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 517.5s of the 3104.8s of remaining time.
	Fine-tuning on the CPU detected. We recommend using a GPU for faster fine-tuning of Chronos.
	Warning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.
	TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
Training timeseries model TemporalFusionTransformer. Training for up to 618.4s of the 3073.6s of remaining time.
	-0.0307       = Validation score (-WQL)
	407.03  s     = Training runtime
	0.04    s     = Validation (prediction) runtime
Training timeseries model DeepAR. Training for up to 688.8s of the 2666.5s of remaining time.
	-0.0404       = Validation score (-WQL)
	175.95  s     = Training runtime
	0.13    s     = Validation (prediction) runtime
Training timeseries model PatchTST. Training for up to 945.2s of the 2490.4s of remaining time.
	-0.0361       = Validation score (-WQL)
	91.12   s     = Training runtime
	0.03    s     = Validation (prediction) runtime
Training timeseries model TiDE. Training for up to 1799.2s of the 2399.2s of remaining time.
	-0.0322       = Validation score (-WQL)
	329.01  s     = Training runtime
	0.03    s     = Validation (prediction) runtime
Fitting simple weighted ensemble.
	Ensemble weights: {'DirectTabular': 0.33, 'PatchTST': 0.15, 'RecursiveTabular': 0.07, 'TemporalFusionTransformer': 0.23, 'TiDE': 0.22}
	-0.0237       = Validation score (-WQL)
	0.50    s     = Training runtime
	1.59    s     = Validation (prediction) runtime
Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'ChronosZeroShot[bolt_base]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE', 'WeightedEnsemble']
Total runtime: 1128.01 s
Best model: WeightedEnsemble
Best model score: -0.0237
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Loading predictor from path D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\AutoML_lag\modelo
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
