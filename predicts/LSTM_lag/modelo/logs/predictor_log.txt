Frequency 'H' stored as 'h'
Beginning AutoGluon training... Time limit = 3600s
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       1.17 GB / 15.93 GB (7.4%)
Disk Space Avail:   690.51 GB / 931.46 GB (74.1%)
===================================================
Setting presets to: medium_quality

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.

Provided data contains following columns:
	target: 'EnergyNormalized'
	known_covariates:
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
	static_features:
		categorical:        []
		continuous (float): ['PopulationDensity']

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================

Starting training. Start time is 2025-05-23 09:01:18
Models that will be trained: ['DeepAR']
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	module 'sympy' has no attribute 'printing'
Not fitting ensemble as no models were successfully trained.
Training complete. Models trained: []
Total runtime: 0.97 s
Trainer has no fit models that can predict.
Warning: path already exists! This predictor may overwrite an existing predictor! path="predicts/LSTM_lag/modelo"
Frequency 'H' stored as 'h'
Frequency 'H' stored as 'h'
Beginning AutoGluon training... Time limit = 3600s
Beginning AutoGluon training... Time limit = 3600s
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       1.72 GB / 15.93 GB (10.8%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       1.72 GB / 15.93 GB (10.8%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.

Provided data contains following columns:

Provided data contains following columns:
	target: 'EnergyNormalized'
	target: 'EnergyNormalized'
	known_covariates:
	known_covariates:
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
	static_features:
	static_features:
		categorical:        []
		categorical:        []
		continuous (float): ['PopulationDensity']
		continuous (float): ['PopulationDensity']

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================
===================================================

Starting training. Start time is 2025-05-23 09:04:37

Starting training. Start time is 2025-05-23 09:04:37
Models that will be trained: ['DeepAR']
Models that will be trained: ['DeepAR']
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	module 'sympy' has no attribute 'printing'
	module 'sympy' has no attribute 'printing'
Not fitting ensemble as no models were successfully trained.
Not fitting ensemble as no models were successfully trained.
Training complete. Models trained: []
Training complete. Models trained: []
Total runtime: 0.06 s
Total runtime: 0.06 s
Trainer has no fit models that can predict.
Trainer has no fit models that can predict.
Loading predictor from path D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\ARIMA_lag\modelo
Loading predictor from path D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\ARIMA_lag\modelo
Model not specified in predict, will default to the model with the best validation score: ARIMA
Model not specified in predict, will default to the model with the best validation score: ARIMA
Warning: path already exists! This predictor may overwrite an existing predictor! path="predicts/LSTM_lag/modelo"
Warning: path already exists! This predictor may overwrite an existing predictor! path="predicts/LSTM_lag/modelo"
Frequency 'H' stored as 'h'
Frequency 'H' stored as 'h'
Frequency 'H' stored as 'h'
Beginning AutoGluon training... Time limit = 3600s
Beginning AutoGluon training... Time limit = 3600s
Beginning AutoGluon training... Time limit = 3600s
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.13 GB / 15.93 GB (13.4%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.13 GB / 15.93 GB (13.4%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.13 GB / 15.93 GB (13.4%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:
	target: 'EnergyNormalized'
	target: 'EnergyNormalized'
	target: 'EnergyNormalized'
	known_covariates:
	known_covariates:
	known_covariates:
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
	static_features:
	static_features:
	static_features:
		categorical:        []
		categorical:        []
		categorical:        []
		continuous (float): ['PopulationDensity']
		continuous (float): ['PopulationDensity']
		continuous (float): ['PopulationDensity']

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================
===================================================
===================================================

Starting training. Start time is 2025-05-23 09:31:17

Starting training. Start time is 2025-05-23 09:31:17

Starting training. Start time is 2025-05-23 09:31:17
Models that will be trained: ['DeepAR']
Models that will be trained: ['DeepAR']
Models that will be trained: ['DeepAR']
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	module 'sympy' has no attribute 'printing'
	module 'sympy' has no attribute 'printing'
	module 'sympy' has no attribute 'printing'
Not fitting ensemble as no models were successfully trained.
Not fitting ensemble as no models were successfully trained.
Not fitting ensemble as no models were successfully trained.
Training complete. Models trained: []
Training complete. Models trained: []
Training complete. Models trained: []
Total runtime: 0.05 s
Total runtime: 0.05 s
Total runtime: 0.05 s
Trainer has no fit models that can predict.
Trainer has no fit models that can predict.
Trainer has no fit models that can predict.
Warning: path already exists! This predictor may overwrite an existing predictor! path="predicts/LSTM_lag/modelo"
Warning: path already exists! This predictor may overwrite an existing predictor! path="predicts/LSTM_lag/modelo"
Warning: path already exists! This predictor may overwrite an existing predictor! path="predicts/LSTM_lag/modelo"
Beginning AutoGluon training... Time limit = 3600s
Beginning AutoGluon training... Time limit = 3600s
Beginning AutoGluon training... Time limit = 3600s
Beginning AutoGluon training... Time limit = 3600s
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.28 GB / 15.93 GB (14.3%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.28 GB / 15.93 GB (14.3%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.28 GB / 15.93 GB (14.3%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.28 GB / 15.93 GB (14.3%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:
	target: 'EnergyNormalized'
	target: 'EnergyNormalized'
	target: 'EnergyNormalized'
	target: 'EnergyNormalized'
	known_covariates:
	known_covariates:
	known_covariates:
	known_covariates:
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
	static_features:
	static_features:
	static_features:
	static_features:
		categorical:        []
		categorical:        []
		categorical:        []
		categorical:        []
		continuous (float): ['PopulationDensity']
		continuous (float): ['PopulationDensity']
		continuous (float): ['PopulationDensity']
		continuous (float): ['PopulationDensity']

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================
===================================================
===================================================
===================================================

Starting training. Start time is 2025-05-23 09:32:42

Starting training. Start time is 2025-05-23 09:32:42

Starting training. Start time is 2025-05-23 09:32:42

Starting training. Start time is 2025-05-23 09:32:42
Models that will be trained: ['DeepAR']
Models that will be trained: ['DeepAR']
Models that will be trained: ['DeepAR']
Models that will be trained: ['DeepAR']
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	module 'sympy' has no attribute 'printing'
	module 'sympy' has no attribute 'printing'
	module 'sympy' has no attribute 'printing'
	module 'sympy' has no attribute 'printing'
Not fitting ensemble as no models were successfully trained.
Not fitting ensemble as no models were successfully trained.
Not fitting ensemble as no models were successfully trained.
Not fitting ensemble as no models were successfully trained.
Training complete. Models trained: []
Training complete. Models trained: []
Training complete. Models trained: []
Training complete. Models trained: []
Total runtime: 0.06 s
Total runtime: 0.06 s
Total runtime: 0.06 s
Total runtime: 0.06 s
Trainer has no fit models that can predict.
Trainer has no fit models that can predict.
Trainer has no fit models that can predict.
Trainer has no fit models that can predict.
Warning: path already exists! This predictor may overwrite an existing predictor! path="predicts/LSTM_lag/modelo"
Warning: path already exists! This predictor may overwrite an existing predictor! path="predicts/LSTM_lag/modelo"
Warning: path already exists! This predictor may overwrite an existing predictor! path="predicts/LSTM_lag/modelo"
Warning: path already exists! This predictor may overwrite an existing predictor! path="predicts/LSTM_lag/modelo"
Beginning AutoGluon training... Time limit = 3600s
Beginning AutoGluon training... Time limit = 3600s
Beginning AutoGluon training... Time limit = 3600s
Beginning AutoGluon training... Time limit = 3600s
Beginning AutoGluon training... Time limit = 3600s
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.20 GB / 15.93 GB (13.8%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.20 GB / 15.93 GB (13.8%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.20 GB / 15.93 GB (13.8%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.20 GB / 15.93 GB (13.8%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       2.20 GB / 15.93 GB (13.8%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality
Setting presets to: medium_quality

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:

Provided data contains following columns:
	target: 'EnergyNormalized'
	target: 'EnergyNormalized'
	target: 'EnergyNormalized'
	target: 'EnergyNormalized'
	target: 'EnergyNormalized'
	known_covariates:
	known_covariates:
	known_covariates:
	known_covariates:
	known_covariates:
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
	past_covariates:
	past_covariates:
	past_covariates:
	past_covariates:
	past_covariates:
		categorical:        ['Season']
		categorical:        ['Season']
		categorical:        ['Season']
		categorical:        ['Season']
		categorical:        ['Season']
		continuous (float): ['ActiveEnergy(kWh)', 'Day', 'Month', 'Year', 'IsWeekend', 'IsHoliday', ...]
		continuous (float): ['ActiveEnergy(kWh)', 'Day', 'Month', 'Year', 'IsWeekend', 'IsHoliday', ...]
		continuous (float): ['ActiveEnergy(kWh)', 'Day', 'Month', 'Year', 'IsWeekend', 'IsHoliday', ...]
		continuous (float): ['ActiveEnergy(kWh)', 'Day', 'Month', 'Year', 'IsWeekend', 'IsHoliday', ...]
		continuous (float): ['ActiveEnergy(kWh)', 'Day', 'Month', 'Year', 'IsWeekend', 'IsHoliday', ...]
	static_features:
	static_features:
	static_features:
	static_features:
	static_features:
		categorical:        []
		categorical:        []
		categorical:        []
		categorical:        []
		categorical:        []
		continuous (float): ['PopulationDensity']
		continuous (float): ['PopulationDensity']
		continuous (float): ['PopulationDensity']
		continuous (float): ['PopulationDensity']
		continuous (float): ['PopulationDensity']

AutoGluon will ignore following non-numeric/non-informative columns:

AutoGluon will ignore following non-numeric/non-informative columns:

AutoGluon will ignore following non-numeric/non-informative columns:

AutoGluon will ignore following non-numeric/non-informative columns:

AutoGluon will ignore following non-numeric/non-informative columns:
	ignored covariates:      ['Date']
	ignored covariates:      ['Date']
	ignored covariates:      ['Date']
	ignored covariates:      ['Date']
	ignored covariates:      ['Date']

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================
===================================================
===================================================
===================================================
===================================================

Starting training. Start time is 2025-05-23 09:36:08

Starting training. Start time is 2025-05-23 09:36:08

Starting training. Start time is 2025-05-23 09:36:08

Starting training. Start time is 2025-05-23 09:36:08

Starting training. Start time is 2025-05-23 09:36:08
Models that will be trained: ['DeepAR']
Models that will be trained: ['DeepAR']
Models that will be trained: ['DeepAR']
Models that will be trained: ['DeepAR']
Models that will be trained: ['DeepAR']
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
Training timeseries model DeepAR. Training for up to 3600.0s of the 3600.0s of remaining time.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	Warning: Exception caused DeepAR to fail during training... Skipping this model.
	module 'sympy' has no attribute 'printing'
	module 'sympy' has no attribute 'printing'
	module 'sympy' has no attribute 'printing'
	module 'sympy' has no attribute 'printing'
	module 'sympy' has no attribute 'printing'
Not fitting ensemble as no models were successfully trained.
Not fitting ensemble as no models were successfully trained.
Not fitting ensemble as no models were successfully trained.
Not fitting ensemble as no models were successfully trained.
Not fitting ensemble as no models were successfully trained.
Training complete. Models trained: []
Training complete. Models trained: []
Training complete. Models trained: []
Training complete. Models trained: []
Training complete. Models trained: []
Total runtime: 0.05 s
Total runtime: 0.05 s
Total runtime: 0.05 s
Total runtime: 0.05 s
Total runtime: 0.05 s
Trainer has no fit models that can predict.
Trainer has no fit models that can predict.
Trainer has no fit models that can predict.
Trainer has no fit models that can predict.
Trainer has no fit models that can predict.
Beginning AutoGluon training... Time limit = 3600s
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          12
GPU Count:          0
Memory Avail:       3.78 GB / 15.93 GB (23.7%)
Disk Space Avail:   690.50 GB / 931.46 GB (74.1%)
===================================================
Setting presets to: medium_quality

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': 'h',
 'hyperparameters': {'DeepAR': {'context_length': 168,
                                'dropout': 0.1,
                                'epochs': 20,
                                'hidden_size': 64,
                                'learning_rate': 0.001,
                                'num_layers': 2,
                                'target_scaler': 'standard'}},
 'known_covariates_names': ['Hour',
                            'TimeOfDay',
                            'Temperature',
                            'DayOfTheWeek',
                            'lag_168'],
 'num_val_windows': 1,
 'prediction_length': 24,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'EnergyNormalized',
 'time_limit': 3600,
 'verbosity': 2}

Provided train_data has 7824 rows, 1 time series. Median time series length is 7824 (min=7824, max=7824). 
Provided tuning_data has 7848 rows, 1 time series. Median time series length is 7848 (min=7848, max=7848). 
	Setting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.

Provided data contains following columns:
	target: 'EnergyNormalized'
	known_covariates:
		categorical:        ['TimeOfDay', 'DayOfTheWeek']
		continuous (float): ['Hour', 'Temperature', 'lag_168']
	past_covariates:
		categorical:        ['Season']
		continuous (float): ['ActiveEnergy(kWh)', 'Day', 'Month', 'Year', 'IsWeekend', 'IsHoliday', ...]
	static_features:
		categorical:        []
		continuous (float): ['PopulationDensity']

AutoGluon will ignore following non-numeric/non-informative columns:
	ignored covariates:      ['Date']

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================

Starting training. Start time is 2025-05-23 09:40:13
Models that will be trained: ['DeepAR']
Training timeseries model DeepAR. Training for up to 3597.7s of the 3597.7s of remaining time.
	-0.0280       = Validation score (-WQL)
	113.41  s     = Training runtime
	0.29    s     = Validation (prediction) runtime
Not fitting ensemble as only 1 model was trained.
Training complete. Models trained: ['DeepAR']
Total runtime: 113.74 s
Best model: DeepAR
Best model score: -0.0280
Model not specified in predict, will default to the model with the best validation score: DeepAR
Beginning AutoGluon training... Time limit = 3600s
AutoGluon will save models to 'D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo'
Loading predictor from path D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\LSTM_lag\modelo
Model not specified in predict, will default to the model with the best validation score: DeepAR
Loading predictor from path D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\CNN_lag\modelo
Model not specified in predict, will default to the model with the best validation score: WaveNet
Loading predictor from path D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\XGBoost_lag\modelo
Model not specified in predict, will default to the model with the best validation score: DirectTabular
Loading predictor from path D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\XT_lag\modelo
Model not specified in predict, will default to the model with the best validation score: DirectTabular
Loading predictor from path D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\NARX_lag\modelo
Model not specified in predict, will default to the model with the best validation score: DirectTabular
Loading predictor from path D:\githubProjects\previsao-do-consumo-energetico-com-ml\predicts\AutoML_lag\modelo
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
